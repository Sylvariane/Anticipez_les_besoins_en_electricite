{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recherche de modèles pour la prédiction de la consommation d'énergie et des émissions de gaz à effets de serre"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sommaire\r\n",
    "\r\n",
    "* [1 - Préparation du notebook](#chapter1)\r\n",
    "\r\n",
    "\r\n",
    "* [2 - Prédiction de la consommation d'énergie](#chapter2)\r\n",
    "    * [2.1. Création d'un modèle \"baseline\"](#section_2_1)\r\n",
    "        * [2.1.1. Préparation des données](#section_2_1_1)\r\n",
    "        * [2.1.2. Création d'un modèle de base](#section_2_1_2)\r\n",
    "        * [2.1.3. Evaluation du modèle avec le jeu de test](#section_2_1_3)\r\n",
    "    * [2.2. Choix du modèle le plus adapté](#section_2_2)\r\n",
    "    * [2.3. Choix des hyperparamètres](#section_2_3)\r\n",
    "    * [2.4. Analyses des erreurs](#section_2_4)\r\n",
    "    * [2.5. Pipeline final](#section_2_5)\r\n",
    "    \r\n",
    "    \r\n",
    "* [3 - Prédiction des émissions de gaz à effets de serre](#chapter3)\r\n",
    "    * [3.1. Préparation des variables](#section_3_1)\r\n",
    "    * [3.2. Création d'un modèle \"baseline\"](#section_3_2)\r\n",
    "    * [3.3. Choix du modèle le plus adapté](#section_3_3)\r\n",
    "    * [3.4. Choix des hyperparamètres](#section_3_4)\r\n",
    "    * [3.5. Analyses des erreurs](#section_3_5)\r\n",
    "    * [3.6. Pipeline finale](#section_3_6)\r\n",
    "    \r\n",
    "* [4 - Prédiction des émissions de gaz à effets de serre à l'aide de l'ENERGYSTAR Score](#chapter4)\r\n",
    "\r\n",
    "* [5 - Conclusion](#chapter5)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Préparation du notebook <a class=\"anchor\" id=\"chapter1\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importation des librairies nécessaires aux tests de modèle\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from scipy import stats\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import sklearn\r\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from yellowbrick.regressor import ResidualsPlot, prediction_error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Paramétrage visuelle de Scikit-Learn\r\n",
    "sklearn.set_config(display=\"diagram\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fonction créée pour l'évaluation des modèles\r\n",
    "def regression_metrics(y_test, y_pred):\r\n",
    "    \"\"\"Function which contains differents metrics about regression\r\n",
    "    Input: y_test, prediction\r\n",
    "    \r\n",
    "    Output: MAE, MSE, RMSE & R² score  \r\n",
    "    \"\"\"\r\n",
    "    mae = mean_absolute_error(y_test, y_pred)\r\n",
    "    mse = mean_squared_error(np.exp(y_test), np.exp(y_pred))\r\n",
    "    rmse = np.sqrt(mse)\r\n",
    "    r_score = r2_score(y_test, y_pred)\r\n",
    "    print(\"MAE: \",mae.round(5))\r\n",
    "    print(\"MSE: \", mse.round(5))\r\n",
    "    print(\"RMSE: \", rmse.round(5))\r\n",
    "    print(\"R²: \", r_score.round(5))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importation des données\r\n",
    "data = pd.read_csv(\"datasets/benchmark_total.csv\")\r\n",
    "data.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Une fois que l'on a chargé les données, on va pouvoir séparer notre jeu de données en deux parties : un train set et un test set. On ne va pas toucher aux données du test set car il nous servia pour la validation de notre modèle. C'est grâce à ce jeu de données que l'on pourra sortir les metrics nécessaires. Dans chaque jeu (entraînement et test), on supprime les variables qui ne seront pas utiles dans notre modélisation. \r\n",
    "\r\n",
    "Tout d'abord, on supprime les variables que l'on cherche à prédire. \"SiteEnergyUse(kBtu)\" et \"TotalGHGEmissions\" sont les deux cibles que l'on cherche à prédire, on les mettra donc dans une variable y. Ensuite, notre jeu de données comporte de nombreuses valeurs : certaines sont d'origine et d'autres proviennent d'une opération de feature engineering. On va donc supprimer les doublons pour éviter la fuite de donneés. \r\n",
    "- Le score ENERGYSTAR est enlevé car on ne va pas s'en servir pour prédire la consommation d'énergie. On regardera son impact dans la prédiction des effets de gaz à effets de serre dans un second temps. \r\n",
    "- L'âge et l'année de construction des bâtiments sont supprimées. On va leur préférence l'utilisation d'une variable catégorielle en 4 modalités pour avoir cette information. La construction de cette variable est expliquée dans la partie \"Analyse\".\r\n",
    "- Le nombre de bâtiments est supprimé et est remplacé par une variable catégorielle qui indique s'il y a 0, 1 ou 2 et plus de bâtiments sur la propriété. Ainsi la compréhension du modèle sera simpligiée par la suite.\r\n",
    "- Pour le nombre d'étages, la logique est identique à celle du nombre de bâtiments. Ces simplifications permettent aussi de palier à la \"mauvaise\" distribution de nos variables. \r\n",
    "- Le quartier est supprimé ainsi que la longitude et la latitude. Ces informations concernent la localisation de la propriété. L'information de la localisation se trouve dans la variable \"Cluster\". Les explications de la création de cette variable se trouvent dans le notebook d'analyse. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Création d'un jeu d'entrainement (train set) et d'un jeu de test (test set)\r\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Clusters\"])\r\n",
    "\r\n",
    "y_train = train_set[\"SiteEnergyUse(kBtu)\"].values\r\n",
    "y_test = test_set[\"SiteEnergyUse(kBtu)\"].values\r\n",
    "X_train = train_set.drop([\"SiteEnergyUse(kBtu)\", \"TotalGHGEmissions\", \"ENERGYSTARScore\", \"Age\", \"YearBuilt\", \"NumberofBuildings\", \"NumberofFloors\", \"Neighborhood\", \"Latitude\", \"Longitude\"], axis=1)\r\n",
    "X_test = test_set.drop([\"SiteEnergyUse(kBtu)\", \"TotalGHGEmissions\", \"ENERGYSTARScore\", \"Age\", \"YearBuilt\", \"NumberofBuildings\", \"NumberofFloors\",\"Neighborhood\", \"Latitude\", \"Longitude\"], axis=1)\r\n",
    "\r\n",
    "y_train = y_train.reshape(-1, 1)\r\n",
    "y_test = y_test.reshape(-1, 1)\r\n",
    "\r\n",
    "y_train = np.log(y_train)\r\n",
    "y_test = np.log(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Prédiction de la consommation d'énergie <a class=\"anchor\" id=\"chapter2\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1) Création d'un modèle \"baseline\" <a class=\"anchor\" id=\"section_2_1\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.1) Préparation des données <a class=\"anchor\" id=\"section_2_1_1\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On sépare les variables catégorielles des variables numériques. Pour pouvoir leur appliquer des traitements différents qui seront enregistrés dans un pipeline de préprocessing. \r\n",
    "Pour les variables catégorielles : \r\n",
    "- On complète les valeurs manquantes par le mode (celle qui est le plus fréquent)\r\n",
    "- On les transforme via un OneHotEncoding. Ainsi, elles seront transformées en valeur numérique compréhensible par notre modèles.\r\n",
    "\r\n",
    "Pour les variables numériques : \r\n",
    "- On complète les valeurs manquantes par la médiane. Il y a une grande diversité dans nos valeurs numériques sans que cela soit des anomalies. Néanmoins, elles vont faire varier notre moyenne, c'est pourquoi on choisi ici la médiane.\r\n",
    "- On standardise nos variables pour enlever les unités qui sont différents (degré Fahrenheit pour Degree Days Heating & Cooling ; sq/ft² pour la taille de la propriété)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Séparation des variables catégorielles et numériques\r\n",
    "\r\n",
    "cat_var = [\"PrimaryPropertyType\", \"NbofFloors\", \"NbofBuildings\", \"HasParking\", \"Clusters\", \"Bins_Age\"]\r\n",
    "num_var = [\"PropertyGFATotal\", \"degreeDaysH\", \"degreeDaysC\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Création d'un pipeline de transformation\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "\r\n",
    "cat_pipe = Pipeline([\r\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\r\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\r\n",
    "])\r\n",
    "\r\n",
    "num_pipe = Pipeline([\r\n",
    "    ('imputer', SimpleImputer(strategy=\"median\", fill_value=\"missing\")),\r\n",
    "    ('scaler', StandardScaler())\r\n",
    "])\r\n",
    "\r\n",
    "preprocessor = ColumnTransformer(\r\n",
    "    transformers=[\r\n",
    "    ('cat', cat_pipe, cat_var),\r\n",
    "    ('num', num_pipe, num_var)\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = preprocessor.fit_transform(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2) Création du modèle de base <a class=\"anchor\" id=\"section_2_1_2\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le modèle choisi pour obtenir une baseline est un modèle naïf. Ce type de modèle prédit la valeur médiane de la variable cible. Le but étant d'avoir une prédiction proche de ce que ferait un modèle basé sur le hasard. Les métriques qui sortiront de ce modèle serviront de base pour évaluer les performances d'autres modèles. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.dummy import DummyRegressor\r\n",
    "\r\n",
    "dummy_reg = DummyRegressor(strategy=\"median\")\r\n",
    "dummy_reg.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.3) Evaluation du modèle avec le jeu de test <a class=\"anchor\" id=\"section_2_1_3\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test = preprocessor.transform(X_test)\r\n",
    "y_pred = dummy_reg.predict(X_test)\r\n",
    "regression_metrics(y_test, y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les métriques choisis sont le (R)MSE, le MAE et le R²."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2) Choix du modèle le plus performant <a class=\"anchor\" id=\"section_2_2\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le problème à laquelle nous sommes confrontés est un problème de régression. On va donc choisir des modèles qui peuvent permettre de faire ce type de prédiction. On va choisir des méthodes linéaires : Régression linéaire et ses variantes régularisées (Ridge et Lasso), le Support Vecteur Machine et des méthodes non linéaires : la régression linéaire ridge avec noyau, des méthodes ensemblistes de bagging (Bagging, Random Forest) et de boosting (AdaBoost, Gradient Boosting, XGBoost)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\r\n",
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "from sklearn.svm import SVR\r\n",
    "from sklearn.kernel_ridge import KernelRidge\r\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\r\n",
    "from sklearn.neural_network import MLPRegressor\r\n",
    "import xgboost as xgb\r\n",
    "\r\n",
    "dummy_reg = DummyRegressor(strategy=\"median\")\r\n",
    "lin_reg = LinearRegression()\r\n",
    "ridge = Ridge(random_state=42)\r\n",
    "lasso = Lasso(random_state=42)\r\n",
    "dt_reg =  DecisionTreeRegressor(random_state=42)\r\n",
    "svm_reg = SVR()\r\n",
    "ridge_kernel = KernelRidge()\r\n",
    "adaboost = AdaBoostRegressor(random_state=42)\r\n",
    "bagging = BaggingRegressor(random_state=42)\r\n",
    "gdboost = GradientBoostingRegressor(random_state=42)\r\n",
    "rdmforest = RandomForestRegressor(random_state=42)\r\n",
    "xgboost = xgb.XGBRegressor(random_state=42)\r\n",
    "mlp_reg = MLPRegressor(random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = [dummy_reg, lin_reg, ridge, lasso, dt_reg, svm_reg, ridge_kernel, adaboost, bagging, gdboost, rdmforest, xgboost, mlp_reg]\r\n",
    "dict_model = {}\r\n",
    "\r\n",
    "for model in model:\r\n",
    "    model.fit(X_train, y_train)\r\n",
    "    y_pred = model.predict(X_test)\r\n",
    "    mae = mean_absolute_error(y_test, y_pred)\r\n",
    "    mse = mean_squared_error(np.exp(y_test), np.exp(y_pred))\r\n",
    "    rmse = mse ** (1/2)\r\n",
    "    r_score = r2_score(y_test, y_pred)\r\n",
    "    dict_model[model] = (mae, mse, rmse, r_score)\r\n",
    "\r\n",
    "eval_model = pd.DataFrame(dict_model, index=[\"MAE\", \"MSE\", \"RMSE\", \"R²\"])\r\n",
    "eval_model.columns = [\"Dummy Regressor\", \"Linear Regression\", \"Ridge\", \"Lasso\", \"DecisionTree\", \"SVM\", \"Ridge Kernel\",\r\n",
    "                      \"AdaBoost\", \"Bagging\", \"GradientBoosting\", \"Random Forest\", \"XGBoost\", \"MLP\"]\r\n",
    "\r\n",
    "eval_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La régression naïve apparaît bien comme l'un des modèles les moins performant. On remarque que les modèles linéaires (Régression linéaire, Régression Ridge/Lasso et SVM) ont un coefficient de détermination aux alentours de 50%. Cela montre que notre problème n'est pas un problème linéaire. On va donc devoir se tourner vers des méthodes linéaires. Le modèle de Random Forest et celui de XGBoost sont ceux qui performent le mieux. On va donc s'orienter sur un modèle de XGBoost. Ces hyperparamètres seront affinés pour obtenir la meilleure performance de notre modèle. \r\n",
    "\r\n",
    "\r\n",
    "**Choix du modèle :** XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3) Choix des hyperparamètres <a class=\"anchor\" id=\"section_2_3\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On va déjà définir quelques paramètres de notre algorithme de XGBoost. On va choisir de booster un modèle se basant sur des arbres de décision (gbtree), on va ensuite fixer sa racine aléatoire et on va fixer un nombre d'arbres dans notre algorithme. On choisit d'utiliser 400 arbres. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xgboost_energy = xgb.XGBRegressor(objective=\"reg:squarederror\", booster=\"gbtree\", \r\n",
    "                               random_state=42, n_estimators=400, tree_method=\"hist\", n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Une fois le modèle choisi, on va pouvoir s'occuper de ces hyperparamètres. On va s'interesser à la profondeur des arbres, à son coût d'apprentissage, à son paramètre gamma et à la méthode utilisée pour le boostraping. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "params = [{\"max_depth\" : [7, 8, 9, 10, 11, 12],\r\n",
    "           \"learning_rate\" : [0.001, 0.005, 0.01, 0.05, 0.1, 0.15],\r\n",
    "           \"gamma\" : [1, 2, 3, 4, 5],\r\n",
    "           \"sampling_method\" : [\"uniform\", \"gradient_based\"]\r\n",
    "}]\r\n",
    "\r\n",
    "grid_search = GridSearchCV(xgboost_energy, params, cv=5,\r\n",
    "                           scoring='neg_mean_squared_error')\r\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search.best_estimator_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notre modèle est enregistré dans la grille de recherche. On va ensuite l'affecter à une variable pour tester ces performances sur un jeu de test. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4) Analyse des erreurs du modèle <a class=\"anchor\" id=\"section_2_4\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Une fois le modèle sélectionné et les hyperparamètres choisis, on va pouvoir évaluer les performances du modèle sur un jeu qu'il n'a pas encore vu : le jeu de test. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_model_energy = grid_search.best_estimator_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_importance(final_model_energy)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_predictions_energy = final_model_energy.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "regression_metrics(y_test, final_predictions_energy)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualizer = ResidualsPlot(final_model_energy)\r\n",
    "visualizer.fit(X_train, y_train.reshape(2596,))\r\n",
    "visualizer.score(X_test, y_test.reshape(649,))\r\n",
    "visualizer.show();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les distributions des résidus du jeu d'entraînement et du jeu de test semblent suivre une loi normale. La distribution des résidus du jeu de test est plus aplatie. Cela peut s'expliquer par le nombre de données moins importants (seulement 20% de l'échantillon). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualizer = prediction_error(final_model_energy, X_train, y_train, X_test, y_test);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5) Pipeline du modèle pour l'estimation de la consommation d'énergie <a class=\"anchor\" id=\"section_2_5\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline\r\n",
    "\r\n",
    "full_pipeline_energy = Pipeline([\r\n",
    "    (\"preprocessing\", preprocessor),\r\n",
    "    (\"model\", final_model_energy)\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Prédiction des émissions de CO2 <a class=\"anchor\" id=\"chapter3\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour la prédiction des émissions de CO2, la méthode est la même que pour la prédiction de la consommation d'énergie."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1) Préparation des variables <a class=\"anchor\" id=\"section_3_1\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Clusters\"])\r\n",
    "\r\n",
    "y_train = train_set[\"TotalGHGEmissions\"].values\r\n",
    "y_test = test_set[\"TotalGHGEmissions\"].values\r\n",
    "X_train = train_set.drop([\"SiteEnergyUse(kBtu)\", \"TotalGHGEmissions\", \"ENERGYSTARScore\", \"YearBuilt\", \"Age\", \"NumberofFloors\", \"NumberofBuildings\", \"Neighborhood\", \"Latitude\", \"Longitude\"], axis=1)\r\n",
    "X_test = test_set.drop([\"SiteEnergyUse(kBtu)\", \"TotalGHGEmissions\", \"ENERGYSTARScore\", \"YearBuilt\", \"Age\", \"NumberofFloors\", \"NumberofBuildings\", 'Neighborhood', \"Latitude\", \"Longitude\"], axis=1)\r\n",
    "\r\n",
    "y_train = y_train.reshape(-1, 1)\r\n",
    "y_test = y_test.reshape(-1, 1)\r\n",
    "\r\n",
    "y_train = np.log(y_train)\r\n",
    "y_test = np.log(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = preprocessor.transform(X_train)\r\n",
    "X_test = preprocessor.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2) Création d'un modèle de base <a class=\"anchor\" id=\"section_3_2\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dummy_reg = DummyRegressor(strategy=\"median\")\r\n",
    "dummy_reg.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = dummy_reg.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "regression_metrics(y_test, y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3) Choix du modèle <a class=\"anchor\" id=\"section_3_3\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dummy_reg = DummyRegressor(strategy=\"median\")\r\n",
    "lin_reg = LinearRegression()\r\n",
    "ridge = Ridge(random_state=42)\r\n",
    "lasso = Lasso(random_state=42)\r\n",
    "dt_reg =  DecisionTreeRegressor(random_state=42)\r\n",
    "svm_reg = SVR()\r\n",
    "ridge_kernel = KernelRidge()\r\n",
    "adaboost = AdaBoostRegressor(random_state=42)\r\n",
    "bagging = BaggingRegressor(random_state=42)\r\n",
    "gdboost = GradientBoostingRegressor(random_state=42)\r\n",
    "rdmforest = RandomForestRegressor(random_state=42)\r\n",
    "xgboost = xgb.XGBRegressor(random_state=42)\r\n",
    "mlp = MLPRegressor(random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = [dummy_reg, lin_reg, ridge, lasso, dt_reg, svm_reg, ridge_kernel, adaboost, bagging, gdboost, rdmforest, xgboost, mlp]\r\n",
    "dict_model = {}\r\n",
    "\r\n",
    "for model in model:\r\n",
    "    model.fit(X_train, y_train)\r\n",
    "    y_pred = model.predict(X_test)\r\n",
    "    mae = mean_absolute_error(y_test, y_pred)\r\n",
    "    mse = mean_squared_error(np.exp(y_test), np.exp(y_pred))\r\n",
    "    rmse = mse ** (1/2)\r\n",
    "    r_score = r2_score(y_test, y_pred)\r\n",
    "    dict_model[model] = (mae, mse, rmse, r_score)\r\n",
    "\r\n",
    "eval_model = pd.DataFrame(dict_model, index=[\"MAE\", \"MSE\", \"RMSE\", \"R²\"])\r\n",
    "eval_model.columns = [\"Dummy Regressor\", \"Linear Regression\", \"Ridge\", \"Lasso\", \"DecisionTree\", \"SVM\", \"Ridge Kernel\",\r\n",
    "                      \"AdaBoost\", \"Bagging\", \"GradientBoosting\", \"Random Forest\", \"XGBoost\", \"MLP\"]\r\n",
    "\r\n",
    "eval_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Modèle choisi :** XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4) Choix des hyperparamètres <a class=\"anchor\" id=\"section_3_4\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xgboost_co2 = xgb.XGBRegressor(objective=\"reg:squarederror\", booster=\"gbtree\", \r\n",
    "                               random_state=42, n_estimators=400, tree_method=\"hist\", n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = [{\"max_depth\" : [7, 8, 9, 10, 11, 12],\r\n",
    "           \"learning_rate\" : [0.001, 0.005, 0.1, 0.15],\r\n",
    "           \"gamma\" : [1, 2, 3, 4, 5],\r\n",
    "           \"sampling_method\" : [\"uniform\", \"gradient_based\"]\r\n",
    "}]\r\n",
    "\r\n",
    "grid_search = GridSearchCV(xgboost_co2, params, cv=5,\r\n",
    "                           scoring='neg_mean_squared_error')\r\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5) Analyse des erreurs du modèle <a class=\"anchor\" id=\"section_3_5\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_model_co2 = grid_search.best_estimator_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_predictions_co2 = final_model_co2.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "regression_metrics(y_test, final_predictions_co2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualizer = ResidualsPlot(final_model_co2)\r\n",
    "visualizer.fit(X_train, y_train.reshape(2596,))\r\n",
    "visualizer.score(X_test, y_test.reshape(649,))\r\n",
    "visualizer.show();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualizer = prediction_error(final_model_co2, X_train, y_train, X_test, y_test);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6) Pipeline du modèle pour l'estimation des émissions de CO2 <a class=\"anchor\" id=\"section_3_6\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_pipeline_co2 = Pipeline([\r\n",
    "    (\"preprocessing\", preprocessor),\r\n",
    "    (\"model\", final_model_co2)\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 - Prédiction des émissions de CO2 avec ENERGY STAR SCORE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Clusters\"])\r\n",
    "\r\n",
    "y_train = train_set[\"TotalGHGEmissions\"].values\r\n",
    "y_test = test_set[\"TotalGHGEmissions\"].values\r\n",
    "X_train = train_set.drop([\"SiteEnergyUse(kBtu)\", \"TotalGHGEmissions\", \"YearBuilt\", \"Age\", \"NumberofFloors\", \"NumberofBuildings\", \"Neighborhood\", \"Latitude\", \"Longitude\"], axis=1)\r\n",
    "X_test = test_set.drop([\"SiteEnergyUse(kBtu)\", \"TotalGHGEmissions\", \"YearBuilt\", \"Age\", 'Neighborhood', \"NumberofBuildings\", \"NumberofFloors\", \"Latitude\", \"Longitude\"], axis=1)\r\n",
    "\r\n",
    "y_train = y_train.reshape(-1, 1)\r\n",
    "y_test = y_test.reshape(-1, 1)\r\n",
    "\r\n",
    "y_train = np.log(y_train)\r\n",
    "y_test = np.log(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Séparation des variables catégorielles et numériques\r\n",
    "\r\n",
    "cat_var = [\"PrimaryPropertyType\", \"NbofFloors\", \"NbofBuildings\", \"HasParking\", \"Clusters\", \"Bins_Age\"]\r\n",
    "num_var = [\"PropertyGFATotal\", \"degreeDaysH\", \"degreeDaysC\", \"ENERGYSTARScore\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Création d'un pipeline de transformation\r\n",
    "\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "\r\n",
    "cat_pipe = Pipeline([\r\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\r\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\r\n",
    "])\r\n",
    "\r\n",
    "num_pipe = Pipeline([\r\n",
    "    ('imputer', SimpleImputer(strategy=\"median\", fill_value=\"missing\")),\r\n",
    "    ('scaler', StandardScaler())\r\n",
    "])\r\n",
    "\r\n",
    "preprocessor = ColumnTransformer(\r\n",
    "    transformers=[\r\n",
    "    ('cat', cat_pipe, cat_var),\r\n",
    "    ('num', num_pipe, num_var)\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = preprocessor.fit_transform(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dummy_reg = DummyRegressor(strategy=\"median\")\r\n",
    "dummy_reg.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test = preprocessor.transform(X_test)\r\n",
    "y_pred = dummy_reg.predict(X_test)\r\n",
    "regression_metrics(y_test, y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dummy_reg = DummyRegressor(strategy=\"median\")\r\n",
    "lin_reg = LinearRegression()\r\n",
    "ridge = Ridge(random_state=42)\r\n",
    "lasso = Lasso(random_state=42)\r\n",
    "dt_reg =  DecisionTreeRegressor(random_state=42)\r\n",
    "svm_reg = SVR()\r\n",
    "ridge_kernel = KernelRidge()\r\n",
    "adaboost = AdaBoostRegressor(random_state=42)\r\n",
    "bagging = BaggingRegressor(random_state=42)\r\n",
    "gdboost = GradientBoostingRegressor(random_state=42)\r\n",
    "rdmforest = RandomForestRegressor(random_state=42)\r\n",
    "xgboost = xgb.XGBRegressor(random_state=42)\r\n",
    "mlp_reg = MLPRegressor(random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = [dummy_reg, lin_reg, ridge, lasso, dt_reg, svm_reg, ridge_kernel, adaboost, bagging, gdboost, rdmforest, xgboost, mlp]\r\n",
    "dict_model = {}\r\n",
    "\r\n",
    "for model in model:\r\n",
    "    model.fit(X_train, y_train)\r\n",
    "    y_pred = model.predict(X_test)\r\n",
    "    mae = mean_absolute_error(y_test, y_pred)\r\n",
    "    mse = mean_squared_error(np.exp(y_test), np.exp(y_pred))\r\n",
    "    rmse = mse ** (1/2)\r\n",
    "    r_score = r2_score(y_test, y_pred)\r\n",
    "    dict_model[model] = (mae, mse, rmse, r_score)\r\n",
    "\r\n",
    "eval_model = pd.DataFrame(dict_model, index=[\"MAE\", \"MSE\", \"RMSE\", \"R²\"])\r\n",
    "eval_model.columns = [\"Dummy Regressor\", \"Linear Regression\", \"Ridge\", \"Lasso\", \"DecisionTree\", \"SVM\", \"Ridge Kernel\",\r\n",
    "                      \"AdaBoost\", \"Bagging\", \"GradientBoosting\", \"Random Forest\", \"XGBoost\", \"MLP\"]\r\n",
    "\r\n",
    "eval_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xgboost_ghg_energystar = xgb.XGBRegressor(objective=\"reg:squarederror\", booster=\"gbtree\", \r\n",
    "                               random_state=42, n_estimators=400, tree_method=\"hist\", n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = [{\"max_depth\" : [7, 8, 9, 10, 11, 12],\r\n",
    "           \"learning_rate\" : [0.001, 0.005, 0.1, 0.15],\r\n",
    "           \"gamma\" : [1, 2, 3, 4, 5],\r\n",
    "           \"sampling_method\" : [\"uniform\", \"gradient_based\"]\r\n",
    "}]\r\n",
    "\r\n",
    "grid_search = GridSearchCV(xgboost_co2, params, cv=5,\r\n",
    "                           scoring='neg_mean_squared_error')\r\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grid_search.best_params_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_model_ghg = grid_search.best_estimator_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_prediction_ghg = final_model_ghg.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "regression_metrics(y_test, final_prediction_ghg)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualizer = ResidualsPlot(final_model_ghg)\r\n",
    "visualizer.fit(X_train, y_train.reshape(2596,))\r\n",
    "visualizer.score(X_test, y_test.reshape(649,))\r\n",
    "visualizer.show();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualizer = prediction_error(final_model_ghg, X_train, y_train, X_test, y_test);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_pipeline_ghg = Pipeline([\r\n",
    "    (\"preprocessing\", preprocessor),\r\n",
    "    (\"model\", final_model_ghg)\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 - Conclusion <a class=\"anchor\" id=\"chapter1\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A l'aide des informations dont l'on dispose, il est possible de prédire la consommation d'énergie et les émissions de CO2. Pour effectuer cela, il faudra mettre en place deux modèles de RandomForest. Les deux modèles finaux sont présents dans le script Python fourni. \n",
    "\n",
    "A l'intérieur, on retrouve l'entraînement et l'enregistrement des modèles. L'avantage du choix d'un modèle comme le Random Forest permet de ne pas transformer la variable cible via une transformation logarithmique. Pour les besoins de la recherche d'un modèle le plus performant possible, il était nécessaire de le faire car certains modèles sont influencés par la non-normalité de la variable cible. En revanche une fois l'étude réalisée, cela n'avait plus aucune raison d'être. C'est pourquoi le script Python des deux modèles finaux ne contient pas de transformation logarithme. Ainsi les modèles prédisent directement la consommation en énergie et les émissions de gaz à effets de serre dans l'unité que l'on a identifié dans nos données initiales. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "db691860837755bfd223dbb19bf59c90fd58faf4b85973e04a456a30cef6b76b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}